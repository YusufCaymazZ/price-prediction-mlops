# Price Prediction E2E ML Pipeline

Production-ready end-to-end machine learning pipeline for price prediction with automated training, deployment, and monitoring.

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Sources                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Prefect Orchestration Layer                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  ETL Flow    â”‚  â”‚ Training Flowâ”‚  â”‚  Drift Check â”‚      â”‚
â”‚  â”‚   (Cron)     â”‚  â”‚    (CT)      â”‚  â”‚     Flow     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Training Pipeline                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Data Processing â†’ Feature Eng â†’ Model Training      â”‚   â”‚
â”‚  â”‚  â†’ Validation â†’ MLflow Tracking â†’ Model Registry     â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    MLflow Registry                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚    Staging   â”‚â†’ â”‚  Production  â”‚â† â”‚   Archive    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Model Serving (FastAPI)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  /predict  /health  /metrics  /model-info           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Monitoring & Observability                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  Prometheus  â”‚â†’ â”‚   Grafana    â”‚â† â”‚  PostgreSQL  â”‚      â”‚
â”‚  â”‚   (Metrics)  â”‚  â”‚ (Dashboards) â”‚  â”‚   (Logs)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (v2.0+)
- Make (optional, for convenience)
- 8GB+ RAM recommended

### Initial Setup

```bash
# Clone repository
git clone <your-repo>
cd price-prediction

# Setup everything (one command)
make setup

# Or manually:
cp .env.example .env
docker-compose build
docker-compose up -d
make init-db
make init-prefect
```

### Access Services

After startup, access the following services:

| Service | URL | Credentials |
|---------|-----|-------------|
| **MLflow UI** | http://localhost:5000 | - |
| **Prefect UI** | http://localhost:4200 | - |
| **API Docs** | http://localhost:8000/docs | - |
| **Grafana** | http://localhost:3000 | admin / your_grafana_password |
| **Prometheus** | http://localhost:9090 | - |
| **MinIO** | http://localhost:9001 | minioadmin / your_minio_password |

## ğŸ“¦ Services

### Core Services

1. **PostgreSQL** - Central database for all metadata
2. **MLflow** - Experiment tracking and model registry
3. **Prefect Server & Worker** - Workflow orchestration
4. **Training Service** - Model training pipeline
5. **Serving Service** - FastAPI model serving
6. **Redis** - Caching and queue management

### Monitoring Stack

7. **Prometheus** - Metrics collection
8. **Grafana** - Visualization dashboards
9. **MinIO** - S3-compatible artifact storage (optional)

## ğŸ”§ Common Operations

### Training

```bash
# Trigger manual training
make dev-train

# Check training logs
make logs-training

# Access training shell
make shell-training
```

### Serving

```bash
# Check serving logs
make logs-serving

# Scale serving instances
make scale-serving n=4

# Test prediction endpoint
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"features": [1.0, 2.0, 3.0]}'
```

### Monitoring

```bash
# Check all service health
make health

# Open Grafana dashboards
make dashboards

# Open Prometheus
make metrics

# View MLflow experiments
make mlflow-ui
```

### Database Operations

```bash
# Create backup
make backup-db

# Restore from backup
make restore-db

# Access database shell
docker-compose exec postgres psql -U mlops -d mlops_db
```

## ğŸ“Š Database Schema

The system uses PostgreSQL with the following schemas:

- **price_data** - Raw and processed feature data
- **model_registry** - Model metadata and deployment history
- **monitoring** - Predictions, metrics, and drift detection

Key tables:
- `price_data.raw_data` - Incoming raw data
- `price_data.processed_features` - Engineered features
- `model_registry.models` - Model versions and metadata
- `monitoring.predictions` - Prediction logs with actuals
- `monitoring.drift_metrics` - Feature drift tracking

## ğŸ”„ Continuous Training (CT)

The pipeline supports automated retraining through Prefect flows:

1. **Data Collection Flow** - Runs on schedule (e.g., daily)
2. **Drift Detection Flow** - Monitors feature/target drift
3. **Training Flow** - Triggers when drift detected or scheduled
4. **Model Promotion Flow** - Validates and promotes models

### Deployment Strategies

- **Manual Rollout** - Requires approval in MLflow UI
- **Auto Rollout** - Automatic if metrics exceed threshold
- **Canary Deployment** - Gradual traffic shifting
- **Blue-Green** - Instant rollback capability

## ğŸ” Security Considerations

### Production Checklist

- [x] Change all default passwords in `.env`
- [ ] Use secrets management (e.g., Docker secrets, Vault)
- [ ] Enable HTTPS/TLS for external endpoints
- [ ] Implement authentication (OAuth2, JWT)
- [ ] Set up network isolation
- [ ] Enable audit logging
- [ ] Configure backup strategies
- [ ] Implement rate limiting
- [ ] Use non-root users in containers

### Recommended Changes

```bash
# Generate secure passwords
openssl rand -base64 32

# Update in .env file
POSTGRES_PASSWORD=<your-secure-password>
GRAFANA_ADMIN_PASSWORD=<your-secure-password>
MINIO_ROOT_PASSWORD=<your-secure-password>
```

## ğŸ“ˆ Performance Tuning

### Serving Optimization

```yaml
# In docker-compose.yml, adjust:
serving:
  deploy:
    resources:
      limits:
        cpus: '2'
        memory: 4G
  environment:
    WORKERS: 4  # CPU cores * 2
```

### Database Optimization

```sql
-- Create additional indexes for your queries
CREATE INDEX idx_custom ON monitoring.predictions(your_column);

-- Partition large tables
CREATE TABLE monitoring.predictions_2024 
  PARTITION OF monitoring.predictions
  FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');
```

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test suite
docker-compose exec training pytest tests/test_training.py -v

# Run with coverage
docker-compose exec training pytest --cov=training tests/
```

## ğŸ“ Development Workflow

1. **Feature Development**
   ```bash
   # Create feature branch
   git checkout -b feature/new-model
   
   # Develop locally
   docker-compose up -d
   make shell-training
   ```

2. **Testing**
   ```bash
   make test
   ```

3. **Deployment**
   ```bash
   git push origin feature/new-model
   # CI/CD pipeline triggers automatically
   ```

## ğŸ› Troubleshooting

### Common Issues

**Services won't start**
```bash
# Check logs
make logs

# Restart services
make restart

# Clean and rebuild
make clean
make setup
```

**Database connection errors**
```bash
# Check database health
docker-compose exec postgres pg_isready -U mlops

# Reinitialize database
make init-db
```

**MLflow artifacts not loading**
```bash
# Check MLflow logs
make logs-mlflow

# Verify artifact location
docker-compose exec mlflow ls -la /mlflow/artifacts
```

## ğŸ“š Project Structure

```
price-prediction/
â”œâ”€â”€ infra/                    # Infrastructure configs
â”‚   â”œâ”€â”€ init-db.sql          # Database initialization
â”‚   â””â”€â”€ mlflow/
â”‚       â””â”€â”€ Dockerfile       # MLflow container
â”œâ”€â”€ airflow-or-prefect/      # Orchestration
â”‚   â”œâ”€â”€ flows/               # Prefect flow definitions
â”‚   â”‚   â”œâ”€â”€ etl.py
â”‚   â”‚   â”œâ”€â”€ training.py
â”‚   â”‚   â””â”€â”€ drift_check.py
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ training/                 # Training pipeline
â”‚   â”œâ”€â”€ train.py             # Main training script
â”‚   â”œâ”€â”€ model/               # Model implementations
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ serving/                  # Model serving
â”‚   â”œâ”€â”€ app.py               # FastAPI application
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ monitoring/               # Observability
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â””â”€â”€ datasources/
â”‚   â””â”€â”€ prometheus/
â”‚       â””â”€â”€ prometheus.yml
â”œâ”€â”€ tests/                    # Test suites
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ docker-compose.yml        # Service definitions
â”œâ”€â”€ Makefile                  # Development commands
â””â”€â”€ .env.example             # Configuration template
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing`)
5. Open Pull Request

## ğŸ“„ License

[Your License Here]

## ğŸ™ Acknowledgments

- MLflow for experiment tracking
- Prefect for workflow orchestration
- FastAPI for high-performance serving
- Prometheus & Grafana for monitoring
